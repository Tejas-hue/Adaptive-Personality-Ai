# Final summary cell
# - List each cleaned file from FinalData/
# - Print dataset name, row count, and head()

from glob import glob

clean_files = sorted(glob(f"{FINAL_DIR}/*_clean.csv"))
if not clean_files:
    print("No cleaned files found in FinalData/.")
else:
    for path in clean_files:
        name = os.path.basename(path)
        try:
            df = read_csv_safely(path)
        except Exception as e:
            print(f"Failed to read {name}: {e}")
            continue
        print("\n" + "="*80)
        print(f"Dataset: {name}")
        print(f"Rows: {len(df)}  Columns: {list(df.columns)}")
        display(df.head(5))
# 7) Additional datasets generic preprocessing
# Example supported: Sentiment140 (text,label) or SST-2 (sentence,label)
# Keep only: text, label
# Steps:
# - For any CSV/TSV found in Data/additional/*, try to infer text+label columns
# - Apply general cleaning -> clean_text, tokens
# - Save each as FinalData/<basename>_clean.csv

additional_dir = f"{DATA_DIR}/additional"
if os.path.isdir(additional_dir):
    for fname in os.listdir(additional_dir):
        if not (fname.endswith(".csv") or fname.endswith(".tsv")):
            continue
        in_path = os.path.join(additional_dir, fname)
        try:
            if fname.endswith(".csv"):
                df = read_csv_safely(in_path)
            else:
                df = read_csv_safely(in_path, sep="\t")
        except Exception as e:
            print(f"Skip {fname}: read error {e}")
            continue

        possible_text_cols = ["text", "sentence", "tweet", "review", "content", "utterance"]
        possible_label_cols = ["label", "sentiment", "polarity", "target", "class"]
        text_col = next((c for c in possible_text_cols if c in df.columns), None)
        label_col = next((c for c in possible_label_cols if c in df.columns), None)
        if text_col is None or label_col is None:
            print(f"Skip {fname}: missing text/label columns")
            continue

        out = pd.DataFrame({
            "text": df[text_col].astype(str),
            "label": df[label_col],
        })
        out["clean_text"] = out["text"].map(clean_text)
        if TOKENIZE:
            out["tokens"] = out["clean_text"].map(lambda t: json.dumps(tokenize_text(t)))

        base = os.path.splitext(fname)[0]
        save_csv_utf8(out, f"{FINAL_DIR}/{base}_clean.csv")
        print(f"Saved: {FINAL_DIR}/{base}_clean.csv (rows={len(out)})")
else:
    print("No additional datasets directory found at Data/additional")
# 6) Emotion Intensity (NRC-EI) preprocessing
# Expected columns: id, tweet, emotion, intensity
# Steps:
# - Read CSV/TSV
# - Ensure intensity is float
# - Clean tweet -> clean_text, tokens
# - Save to FinalData/emointensity_clean.csv

ei_input_csv = f"{DATA_DIR}/nrc_ei.csv"
ei_input_tsv = f"{DATA_DIR}/nrc_ei.tsv"
ei_output = f"{FINAL_DIR}/emointensity_clean.csv"

if file_exists(ei_input_csv) or file_exists(ei_input_tsv):
    if file_exists(ei_input_csv):
        df = read_csv_safely(ei_input_csv)
    else:
        df = read_csv_safely(ei_input_tsv, sep="\t")

    # Detect columns
    possible_id_cols = ["id", "ID"]
    possible_text_cols = ["tweet", "text", "utterance"]
    possible_emotion_cols = ["emotion", "label"]
    possible_intensity_cols = ["intensity", "score", "Intensity"]

    id_col = next((c for c in possible_id_cols if c in df.columns), None)
    text_col = next((c for c in possible_text_cols if c in df.columns), None)
    em_col = next((c for c in possible_emotion_cols if c in df.columns), None)
    intensity_col = next((c for c in possible_intensity_cols if c in df.columns), None)

    if text_col is None or em_col is None or intensity_col is None:
        raise ValueError("Could not find tweet/emotion/intensity columns in NRC-EI dataset")

    out = pd.DataFrame({
        "id": df[id_col] if id_col else range(len(df)),
        "tweet": df[text_col].astype(str),
        "emotion": df[em_col],
        "intensity": pd.to_numeric(df[intensity_col], errors="coerce").astype(float),
    })

    out["clean_text"] = out["tweet"].map(clean_text)
    if TOKENIZE:
        out["tokens"] = out["clean_text"].map(lambda t: json.dumps(tokenize_text(t)))

    save_csv_utf8(out, ei_output)
    print(f"Saved: {ei_output}  (rows={len(out)})")
else:
    print("NRC-EI not found (expected nrc_ei.csv/tsv in Data/")
# 5) Emotion-Stimulus preprocessing
# Expected columns: text, emotion, stimulus
# Keep all three, preprocess only text (stimulus unchanged)
# Steps:
# - Read CSV/TSV
# - Ensure columns present
# - Clean text -> clean_text, tokens
# - Save to FinalData/emotionstimulus_clean.csv

stim_input_csv = f"{DATA_DIR}/emotion_stimulus.csv"
stim_input_tsv = f"{DATA_DIR}/emotion_stimulus.tsv"
stim_output = f"{FINAL_DIR}/emotionstimulus_clean.csv"

if file_exists(stim_input_csv) or file_exists(stim_input_tsv):
    if file_exists(stim_input_csv):
        df = read_csv_safely(stim_input_csv)
    else:
        df = read_csv_safely(stim_input_tsv, sep="\t")

    possible_text_cols = ["text", "sentence", "utterance"]
    possible_label_cols = ["emotion", "label"]
    possible_stim_cols = ["stimulus", "cause", "trigger"]

    text_col = next((c for c in possible_text_cols if c in df.columns), None)
    label_col = next((c for c in possible_label_cols if c in df.columns), None)
    stim_col = next((c for c in possible_stim_cols if c in df.columns), None)

    if not all([text_col, label_col, stim_col]):
        raise ValueError("Could not find text/emotion/stimulus columns in Emotion-Stimulus dataset")

    out = pd.DataFrame({
        "text": df[text_col].astype(str),
        "emotion": df[label_col],
        "stimulus": df[stim_col],
    })

    out["clean_text"] = out["text"].map(clean_text)
    if TOKENIZE:
        out["tokens"] = out["clean_text"].map(lambda t: json.dumps(tokenize_text(t)))

    save_csv_utf8(out, stim_output)
    print(f"Saved: {stim_output}  (rows={len(out)})")
else:
    print("Emotion-Stimulus not found (expected emotion_stimulus.csv/tsv in Data/")
# 4) DailyDialog preprocessing
# Raw data: dialogs with multiple utterances and per-utterance emotion labels
# Keep only: utterance, emotion
# Steps:
# - Read CSV/JSON as available
# - If dialog-level structure, flatten to one row per utterance with associated emotion
# - Clean utterance -> clean_text, tokens
# - Save to FinalData/dailydialog_clean.csv

# We support two common formats:
# - CSV with columns like: dialog_id, utterance, emotion
# - JSON where each item is {"dialog": ["utt1", "utt2", ...], "emotions": [e1, e2, ...]}

dd_input_csv = f"{DATA_DIR}/dailydialog.csv"
dd_input_json = f"{DATA_DIR}/dailydialog.json"
dd_output = f"{FINAL_DIR}/dailydialog_clean.csv"

if file_exists(dd_input_csv) or file_exists(dd_input_json):
    rows = []
    if file_exists(dd_input_csv):
        df = read_csv_safely(dd_input_csv)
        possible_text_cols = ["utterance", "text", "Utterance"]
        possible_label_cols = ["emotion", "label", "Emotion"]
        text_col = next((c for c in possible_text_cols if c in df.columns), None)
        label_col = next((c for c in possible_label_cols if c in df.columns), None)
        if text_col is None or label_col is None:
            raise ValueError("Could not find utterance/emotion columns in DailyDialog CSV")
        tmp = df[[text_col, label_col]].copy()
        tmp.columns = ["utterance", "emotion"]
        rows = tmp.to_dict(orient="records")
    else:
        with open(dd_input_json, "r", encoding="utf-8") as f:
            data = json.load(f)
        # Expect list of dialogs
        for item in data:
            dialog_utts = item.get("dialog") or item.get("utterances") or []
            dialog_emots = item.get("emotions") or []
            for i, utt in enumerate(dialog_utts):
                emo = dialog_emots[i] if i < len(dialog_emots) else None
                rows.append({"utterance": utt, "emotion": emo})

    out = pd.DataFrame(rows)
    out["utterance"] = out["utterance"].astype(str)
    out["clean_text"] = out["utterance"].map(clean_text)
    if TOKENIZE:
        out["tokens"] = out["clean_text"].map(lambda t: json.dumps(tokenize_text(t)))

    save_csv_utf8(out[["utterance", "emotion", "clean_text"] + (["tokens"] if TOKENIZE else [])], dd_output)
    print(f"Saved: {dd_output}  (rows={len(out)})")
else:
    print("DailyDialog not found (expected dailydialog.csv or dailydialog.json in Data/")
# 3) GoEmotions preprocessing
# Expected columns: text, emotion(s)
# Keep only: text and emotion
# If multiple labels, keep multi-label format (comma separated)
# Steps:
# - Read CSV/TSV (auto-detect separator if needed)
# - Standardize to columns: text, emotion
# - Clean text -> clean_text, tokens
# - Save to FinalData/goemotions_clean.csv

goemo_input_csv = f"{DATA_DIR}/goemotions.csv"
goemo_input_tsv = f"{DATA_DIR}/goemotions.tsv"
goemo_output = f"{FINAL_DIR}/goemotions_clean.csv"

if file_exists(goemo_input_csv) or file_exists(goemo_input_tsv):
    if file_exists(goemo_input_csv):
        df = read_csv_safely(goemo_input_csv)
    else:
        df = read_csv_safely(goemo_input_tsv, sep="\t")

    possible_text_cols = ["text", "comment_text", "utterance"]
    text_col = next((c for c in possible_text_cols if c in df.columns), None)

    possible_label_cols = ["emotion", "emotions", "labels", "label"]
    label_col = next((c for c in possible_label_cols if c in df.columns), None)

    if text_col is None or label_col is None:
        raise ValueError("Could not find text/emotion columns in GoEmotions dataset")

    # Ensure labels as comma-separated string if list-like
    def to_comma_labels(x):
        if isinstance(x, list):
            return ",".join(map(str, x))
        try:
            # handle strings that represent lists like "[joy, anger]" or "['joy','anger']"
            if isinstance(x, str) and (x.startswith("[") and x.endswith("]")):
                inner = x[1:-1].strip()
                if not inner:
                    return ""
                parts = re.split(r"\s*,\s*", inner)
                parts = [p.strip("'\" ") for p in parts if p.strip("'\" ")]
                return ",".join(parts)
        except Exception:
            pass
        return str(x)

    out = pd.DataFrame({
        "text": df[text_col].astype(str),
        "emotion": df[label_col].apply(to_comma_labels),
    })

    out["clean_text"] = out["text"].map(clean_text)
    if TOKENIZE:
        out["tokens"] = out["clean_text"].map(lambda t: json.dumps(tokenize_text(t)))

    save_csv_utf8(out, goemo_output)
    print(f"Saved: {goemo_output}  (rows={len(out)})")
else:
    print("GoEmotions not found (expected goemotions.csv or goemotions.tsv in Data/")
# 2) ISEAR preprocessing
# Typical raw columns might include: SIT (situation text), EMOT (label), etc.
# Keep: emotion_label (lowercased) and text
# Steps:
# - Read CSV/TSV (try common separators)
# - Identify label/text columns; fallback names handled
# - Normalize emotion labels to lowercase
# - Apply general cleaning to text -> clean_text, tokens
# - Save to FinalData/isear_clean.csv

isear_input_csv = f"{DATA_DIR}/isear.csv"
isear_input_tsv = f"{DATA_DIR}/isear.tsv"
isear_output = f"{FINAL_DIR}/isear_clean.csv"

if file_exists(isear_input_csv) or file_exists(isear_input_tsv):
    if file_exists(isear_input_csv):
        df = read_csv_safely(isear_input_csv)
    else:
        df = read_csv_safely(isear_input_tsv, sep="\t")

    # Try to detect columns
    possible_text_cols = ["SIT", "text", "SIT_WORDS", "Utterance", "content"]
    possible_label_cols = ["EMOT", "emotion", "label", "Emotion"]

    text_col = next((c for c in possible_text_cols if c in df.columns), None)
    label_col = next((c for c in possible_label_cols if c in df.columns), None)

    if text_col is None or label_col is None:
        raise ValueError("Could not find text/label columns in ISEAR dataset")

    out = pd.DataFrame({
        "emotion_label": df[label_col].astype(str).str.lower().str.strip(),
        "text": df[text_col].astype(str),
    })

    # Clean
    out["clean_text"] = out["text"].map(clean_text)
    if TOKENIZE:
        out["tokens"] = out["clean_text"].map(lambda t: json.dumps(tokenize_text(t)))

    save_csv_utf8(out, isear_output)
    print(f"Saved: {isear_output}  (rows={len(out)})")
else:
    print("ISEAR not found (expected isear.csv or isear.tsv in Data/")
# 1) EmoBank preprocessing
# Raw expected: Data/emobank.csv with columns: id, split, V, A, D, text
# Keep: id, split, V, A, D, text
# Steps:
# - Read CSV (UTF-8 with fallback)
# - Drop rows with missing text
# - Ensure V,A,D are floats
# - Apply general cleaning to text -> clean_text, tokens
# - Save to FinalData/emobank_clean.csv

import math

emobank_input = f"{DATA_DIR}/emobank.csv"
emobank_output = f"{FINAL_DIR}/emobank_clean.csv"

if file_exists(emobank_input):
    df = read_csv_safely(emobank_input)
    # Keep only required columns if present
    required_cols = ["id", "split", "V", "A", "D", "text"]
    existing = [c for c in required_cols if c in df.columns]
    df = df[existing]

    # Drop rows with missing text
    if "text" in df.columns:
        df = df[~df["text"].isna()]

    # Ensure floats for V, A, D when present
    for col in ["V", "A", "D"]:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors="coerce").astype(float)

    # Clean text
    if "text" in df.columns:
        df["clean_text"] = df["text"].map(clean_text)
        if TOKENIZE:
            df["tokens"] = df["clean_text"].map(lambda t: json.dumps(tokenize_text(t)))

    save_csv_utf8(df, emobank_output)
    print(f"Saved: {emobank_output}  (rows={len(df)})")
else:
    print(f"EmoBank not found: {emobank_input}")
# General text preprocessing function (applies to ALL datasets)
# Steps:
# - Lowercase all text
# - Remove leading/trailing whitespace
# - Remove HTML tags
# - Remove URLs, emails, hashtags, mentions
# - Remove numbers and excessive punctuation (keep . ! ?)
# - Normalize whitespace (single spaces only)
# - Tokenize text (optional)
# - Handle encoding issues implicitly (read/save as UTF-8)
from html import unescape

URL_PATTERN = re.compile(r"https?://\S+|www\.\S+", flags=re.IGNORECASE)
EMAIL_PATTERN = re.compile(r"\b[\w\.-]+@[\w\.-]+\.[a-zA-Z]{2,}\b")
MENTION_PATTERN = re.compile(r"@\w+")
HASHTAG_PATTERN = re.compile(r"#\w+")
HTML_TAG_PATTERN = re.compile(r"<[^>]+>")
NUMBERS_PATTERN = re.compile(r"\b\d+\b")
# Keep . ! ? ; remove other excessive punctuation runs to a single char
PUNCT_TO_KEEP = ".!?"
PUNCT_PATTERN = re.compile(rf"[^\w\s{re.escape(PUNCT_TO_KEEP)}]|")
EXCESSIVE_PUNCT_PATTERN = re.compile(rf"([{re.escape(PUNCT_TO_KEEP)}])\1+")

TOKENIZE = True  # set False if you don't want tokenized output

def clean_text(text: str) -> str:
    if not isinstance(text, str):
        return ""
    # Decode HTML entities and remove tags
    text = unescape(text)
    text = HTML_TAG_PATTERN.sub(" ", text)

    # Lowercase & strip
    text = text.lower().strip()

    # Remove URLs, emails, mentions, hashtags
    text = URL_PATTERN.sub(" ", text)
    text = EMAIL_PATTERN.sub(" ", text)
    text = MENTION_PATTERN.sub(" ", text)
    text = HASHTAG_PATTERN.sub(" ", text)

    # Remove numbers
    text = NUMBERS_PATTERN.sub(" ", text)

    # Remove disallowed punctuation; keep . ! ?
    text = re.sub(r"[_`~^<>{}\[\]\\/|%$£€+=*\-]", " ", text)
    text = re.sub(r",|:|;|\(|\)|\"|'", " ", text)

    # Collapse excessive kept punctuation like '!!!' -> '!'
    text = EXCESSIVE_PUNCT_PATTERN.sub(r"\1", text)

    # Normalize whitespace
    text = normalize_whitespace(text)
    return text


def tokenize_text(text: str) -> List[str]:
    # Simple whitespace tokenization; could swap for nltk/spaCy if desired
    if not isinstance(text, str):
        return []
    return [tok for tok in text.split(" ") if tok]
# Setup: imports, paths, and utilities
import os
import re
import json
from typing import List, Optional

import pandas as pd

# Directories (adjust if your data lives elsewhere)
ROOT_DIR = "/workspace"
DATA_DIR = f"{ROOT_DIR}/Data"
FINAL_DIR = f"{ROOT_DIR}/FinalData"

# Ensure output directory exists
os.makedirs(FINAL_DIR, exist_ok=True)


def read_csv_safely(path: str, **kwargs) -> pd.DataFrame:
    """Read a CSV robustly with UTF-8 handling and graceful fallbacks.

    Tries UTF-8 first, then falls back to latin-1 if needed.
    """
    try:
        return pd.read_csv(path, encoding="utf-8", **kwargs)
    except UnicodeDecodeError:
        return pd.read_csv(path, encoding="latin-1", **kwargs)


def save_csv_utf8(df: pd.DataFrame, path: str) -> None:
    """Save DataFrame as UTF-8 CSV without index."""
    df.to_csv(path, index=False, encoding="utf-8")


def file_exists(path: str) -> bool:
    return os.path.isfile(path)


def normalize_whitespace(text: str) -> str:
    return re.sub(r"\s+", " ", text).strip()
